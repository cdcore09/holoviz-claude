# Lumen AI Skill

## Overview

Master AI-powered natural language data exploration with Lumen AI. This skill covers building conversational data analysis applications where users ask questions in plain English and receive visualizations, SQL queries, and insights automatically generated by large language models.

## Dependencies

- lumen >= 0.10.0 (with AI support)
- panel >= 1.3.0
- openai or anthropic or other LLM provider libraries

## What is Lumen AI?

Lumen AI is an open-source, agent-based framework for conversational data exploration that translates natural language queries into:
- SQL queries for database exploration
- Interactive visualizations
- Statistical summaries
- Custom analyses
- Data-driven insights

### Key Features

- **Natural Language Interface**: Ask questions in plain English, get visualizations and answers
- **Multi-LLM Support**: Works with OpenAI, Anthropic, Google, Mistral, local models (Ollama, LlamaCPP)
- **Agent Architecture**: Specialized agents handle different task types (SQL, charts, analyses)
- **Extensible**: Custom agents, tools, and analyses for domain-specific needs
- **Privacy-Focused**: Full local deployment option for sensitive data
- **No Vendor Lock-in**: Switch LLM providers with single configuration change

### Lumen AI vs. Lumen Dashboards

**Lumen Dashboards**: Declarative YAML specifications for building fixed dashboards
**Lumen AI**: Conversational interface with AI-generated queries and visualizations

**Use Lumen AI when**:
- Users need ad-hoc data exploration
- Questions vary and aren't predictable
- Non-technical users need data access
- Reducing analyst backlog
- Enabling self-service analytics

**Use Lumen Dashboards when**:
- Dashboard structure is fixed
- Same visualizations needed repeatedly
- No LLM costs desired
- Full control over outputs needed

## Core Capabilities

### 1. Getting Started

#### Installation

```bash
# Install Lumen with AI support
pip install lumen[ai]

# Install LLM provider (choose one or more)
pip install openai              # OpenAI
pip install anthropic           # Anthropic Claude
pip install mistralai           # Mistral
pip install llama-cpp-python    # Local models
```

#### Basic Launch

```bash
# Set API key
export OPENAI_API_KEY="sk-..."

# Launch with built-in interface
lumen-ai serve

# With pre-loaded dataset
lumen-ai serve https://datasets.holoviz.org/penguins/v1/penguins.csv

# With local file
lumen-ai serve ./data/sales.csv

# With database
lumen-ai serve "postgresql://user:pass@localhost/mydb"
```

### 2. LLM Provider Configuration

#### OpenAI

```bash
export OPENAI_API_KEY="sk-..."
```

```python
import lumen.ai as lmai

# Default: uses gpt-4o-mini
lmai.llm.llm_type = "openai"
lmai.llm.model = "gpt-4o"  # Or gpt-4o-mini, gpt-4-turbo
```

#### Anthropic Claude

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
```

```python
import lumen.ai as lmai

lmai.llm.llm_type = "anthropic"
lmai.llm.model = "claude-3-5-sonnet-20241022"  # Or claude-3-opus, claude-3-haiku
```

#### Mistral

```bash
export MISTRAL_API_KEY="..."
```

```python
import lumen.ai as lmai

lmai.llm.llm_type = "mistral"
lmai.llm.model = "mistral-large-latest"
```

#### Google Gemini

```bash
export GOOGLE_API_KEY="..."
```

```python
import lumen.ai as lmai

lmai.llm.llm_type = "google"
lmai.llm.model = "gemini-1.5-pro"
```

#### Local Models (Ollama)

```bash
# Install and run Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.1

# Or pull other models
ollama pull mistral
ollama pull codellama
```

```python
import lumen.ai as lmai

lmai.llm.llm_type = "ollama"
lmai.llm.model = "llama3.1"
lmai.llm.base_url = "http://localhost:11434"
```

#### Local Models (LlamaCPP)

```bash
pip install llama-cpp-python
```

```python
import lumen.ai as lmai

lmai.llm.llm_type = "llamacpp"
lmai.llm.model_path = "./models/llama-3.1-8B-Instruct.gguf"
```

### 3. Python API Usage

#### Basic Setup

```python
import lumen.ai as lmai
import panel as pn

pn.extension()

# Initialize with dataset
from lumen.sources.duckdb import DuckDBSource

source = DuckDBSource(
    tables=["./data/sales.csv", "./data/inventory.parquet"]
)

# Create AI explorer
ui = lmai.ExplorerUI(
    source=source,
    title="Sales Analytics AI"
)

ui.servable()
```

#### Custom Configuration

```python
import lumen.ai as lmai

# Configure LLM
lmai.llm.llm_type = "anthropic"
lmai.llm.model = "claude-3-5-sonnet-20241022"
lmai.llm.temperature = 0.1  # Lower = more deterministic

# Select specific agents
agents = [
    lmai.agents.TableListAgent,
    lmai.agents.ChatAgent,
    lmai.agents.SQLAgent,
    lmai.agents.hvPlotAgent,
]

# Add custom tools
tools = [
    lmai.tools.DocumentLookup,
    lmai.tools.TableLookup
]

# Create UI
ui = lmai.ExplorerUI(
    source=source,
    agents=agents,
    tools=tools,
    title="Custom Analytics AI",
    accent_color="#00aa41"
)

ui.servable()
```

### 4. Built-in Agents

Lumen AI uses specialized agents for different tasks:

#### TableListAgent

**Purpose**: Provides overview of available tables and their schemas

**Example queries**:
- "What tables are available?"
- "Show me the schema of the sales table"
- "What columns does the customer table have?"

#### ChatAgent

**Purpose**: General conversation, dataset summaries, suggestions

**Example queries**:
- "Give me a summary of this dataset"
- "What interesting questions can I ask?"
- "Explain what this data represents"

#### SQLAgent

**Purpose**: Generates SQL queries from natural language

**Example queries**:
- "Show me total sales by region"
- "What were the top 10 products last month?"
- "Calculate average order value per customer"

**Output**: SQL query + result table

#### hvPlotAgent

**Purpose**: Creates interactive visualizations

**Example queries**:
- "Create a scatter plot of price vs quantity"
- "Show me revenue trend over time by category"
- "Plot a histogram of customer ages"

**Output**: Interactive hvPlot visualization

#### VegaLiteAgent

**Purpose**: Generates publication-quality Vega-Lite specifications

**Example queries**:
- "Create a polished bar chart of sales by region"
- "Make an exportable visualization of trends"

**Output**: Vega-Lite JSON specification + rendered chart

#### AnalysisAgent

**Purpose**: Executes custom domain-specific analyses

**Triggers**: Automatically invoked when custom analyses match query + data

#### SourceAgent

**Purpose**: Handles data upload and import

**Triggered**: When users upload files via UI

### 5. Custom Agents

Create specialized agents for domain-specific tasks:

```python
from lumen.ai.agents import Agent
import param

class SentimentAgent(Agent):
    """Analyze sentiment in text data."""

    requires = param.List(default=["current_source"])
    provides = param.List(default=["sentiment_analysis"])

    purpose = """
    Analyzes sentiment in text columns of the dataset.
    Use when user asks about sentiment, emotions, or tone in text data.
    """

    _prompts = {
        "system": """
        You are a sentiment analysis expert.
        Analyze text data to determine positive, negative, or neutral sentiment.
        Provide confidence scores and key phrases.
        """
    }

    async def respond(self, query: str):
        """Execute sentiment analysis."""
        # Get current data source
        source = self.memory["current_source"]

        # Find text columns
        df = source.get(source.tables[0])
        text_columns = df.select_dtypes(include=['object']).columns

        if not len(text_columns):
            return "No text columns found for sentiment analysis."

        # Perform analysis (simplified example)
        analysis_prompt = f"""
        Analyze sentiment in the '{text_columns[0]}' column.
        Query: {query}

        Provide summary statistics and key insights.
        """

        # Stream response from LLM
        response = ""
        async for chunk in self._stream(analysis_prompt):
            response += chunk
            yield chunk

        # Store in memory
        self.memory["sentiment_analysis"] = response

# Register agent
ui = lmai.ExplorerUI(
    source=source,
    agents=[SentimentAgent, lmai.agents.ChatAgent]
)
```

### 6. Custom Analyses

Define reusable analytical views:

```python
from lumen.ai.analyses import Analysis
from lumen.pipeline import Pipeline
import param
import pandas as pd
import hvplot.pandas

class CustomerSegmentationAnalysis(Analysis):
    """Segment customers by behavior patterns."""

    # Required columns
    columns = param.List(default=[
        'customer_id',
        'total_purchases',
        'avg_order_value',
        'days_since_last_purchase'
    ])

    def __call__(self, pipeline: Pipeline):
        """Execute segmentation analysis."""
        # Get data
        df = pipeline.data

        # Perform segmentation
        df['segment'] = 'Low Value'
        df.loc[
            (df['total_purchases'] > 5) & (df['avg_order_value'] > 100),
            'segment'
        ] = 'High Value'
        df.loc[
            (df['total_purchases'] > 2) & (df['avg_order_value'] > 50) &
            (df['segment'] == 'Low Value'),
            'segment'
        ] = 'Medium Value'

        # Create visualizations
        segment_counts = df['segment'].value_counts()

        pie_chart = segment_counts.hvplot.pie(
            title='Customer Segments',
            height=300,
            width=400
        )

        scatter = df.hvplot.scatter(
            x='total_purchases',
            y='avg_order_value',
            by='segment',
            title='Customer Segmentation',
            height=400,
            width=600
        )

        summary_table = df.groupby('segment').agg({
            'customer_id': 'count',
            'total_purchases': 'mean',
            'avg_order_value': 'mean'
        }).round(2)

        # Return layout
        from lumen.dashboard import Layout
        return Layout(
            title='Customer Segmentation Analysis',
            layout=[[0, 1], [2]],
            views=[pie_chart, scatter, summary_table]
        )

# Register with AnalysisAgent
ui = lmai.ExplorerUI(
    source=source,
    agents=[
        lmai.agents.AnalysisAgent(analyses=[CustomerSegmentationAnalysis])
    ]
)
```

### 7. Custom Tools

Provide agents with additional context and capabilities:

```python
from lumen.ai.tools import Tool
import param
import requests

class ExternalAPITool(Tool):
    """Fetch data from external API."""

    api_url = param.String(default="https://api.example.com")

    @property
    def description(self):
        return """
        Fetches additional data from external API.
        Use when user asks about data not in the current dataset.
        """

    def __call__(self, endpoint: str, params: dict = None):
        """
        Fetch data from API endpoint.

        Args:
            endpoint: API endpoint path
            params: Query parameters

        Returns:
            JSON response data
        """
        url = f"{self.api_url}/{endpoint}"
        response = requests.get(url, params=params)
        return response.json()

# Functional tool (simpler approach)
def get_weather_data(city: str) -> dict:
    """
    Get current weather for a city.

    Args:
        city: City name

    Returns:
        Weather data including temperature, conditions
    """
    # Simplified example
    return {
        "city": city,
        "temperature": 72,
        "conditions": "Sunny"
    }

# Register tools
ui = lmai.ExplorerUI(
    source=source,
    tools=[
        ExternalAPITool,
        lmai.tools.FunctionTool(func=get_weather_data)
    ]
)
```

### 8. Document Context (RAG)

Add documents for context-aware responses:

```python
from lumen.sources.duckdb import DuckDBSource

# Load data tables
source = DuckDBSource(
    tables=[
        "./data/sales.csv",
        "./data/products.parquet"
    ],
    # Add documents for context
    documents=[
        "./docs/data_dictionary.pdf",
        "./docs/business_rules.md",
        "./docs/analysis_guidelines.docx"
    ]
)

ui = lmai.ExplorerUI(
    source=source,
    tools=[lmai.tools.DocumentLookup]  # Enables document search
)
```

**Supported document formats**: PDF, DOCX, TXT, MD, HTML, RST

**How it works**:
1. Documents are chunked and embedded
2. Relevant chunks retrieved based on query
3. Context provided to agents for informed responses

### 9. Memory and Context

Access and manage conversation context:

```python
import lumen.ai as lmai

# Access memory in custom agent
class CustomAgent(Agent):
    async def respond(self, query: str):
        # Read from memory
        source = self.memory.get("current_source")
        previous_sql = self.memory.get("sql_query")

        # Process...

        # Write to memory
        self.memory["custom_result"] = result

        yield result

# Inspect memory during development
ui = lmai.ExplorerUI(source=source)
print(ui.agent_manager.memory.keys())
```

**Common memory keys**:
- `current_source`: Active data source
- `sql_query`: Last generated SQL
- `available_tables`: Table metadata
- `current_table`: Selected table
- Custom keys from agents

### 10. Coordinator Types

Control agent orchestration:

#### DependencyResolver (Default)

Recursively resolves agent dependencies:

```python
ui = lmai.ExplorerUI(
    source=source,
    coordinator="dependency"  # Default
)
```

**Behavior**: Selects agent based on purpose, recursively satisfies requirements

#### Planner

Creates step-by-step execution plans:

```python
ui = lmai.ExplorerUI(
    source=source,
    coordinator="planner"
)
```

**Behavior**: Develops plan upfront, executes steps sequentially

## Complete Example: Business Analytics AI

```python
import lumen.ai as lmai
import panel as pn
from lumen.sources.duckdb import DuckDBSource
from lumen.ai.analyses import Analysis
from lumen.ai.tools import Tool
import param

pn.extension()

# Configure LLM
lmai.llm.llm_type = "anthropic"
lmai.llm.model = "claude-3-5-sonnet-20241022"

# Data source with documents
source = DuckDBSource(
    tables=[
        "./data/sales_2024.parquet",
        "./data/customers.csv",
        "./data/products.csv"
    ],
    documents=[
        "./docs/metrics_definitions.md",
        "./docs/business_glossary.pdf"
    ]
)

# Custom analysis: Cohort retention
class CohortAnalysis(Analysis):
    """Customer cohort retention analysis."""

    columns = param.List(default=[
        'customer_id',
        'signup_date',
        'purchase_date',
        'revenue'
    ])

    def __call__(self, pipeline):
        import pandas as pd
        import hvplot.pandas

        df = pipeline.data

        # Calculate cohorts
        df['signup_month'] = pd.to_datetime(df['signup_date']).dt.to_period('M')
        df['purchase_month'] = pd.to_datetime(df['purchase_date']).dt.to_period('M')
        df['cohort_age'] = (
            df['purchase_month'] - df['signup_month']
        ).apply(lambda x: x.n)

        # Retention matrix
        cohort_data = df.groupby(['signup_month', 'cohort_age']).agg({
            'customer_id': 'nunique'
        }).reset_index()

        cohort_pivot = cohort_data.pivot(
            index='signup_month',
            columns='cohort_age',
            values='customer_id'
        )

        # Calculate retention %
        cohort_size = cohort_pivot[0]
        retention = cohort_pivot.div(cohort_size, axis=0) * 100

        # Visualize
        heatmap = retention.hvplot.heatmap(
            title='Cohort Retention (%)',
            cmap='RdYlGn',
            height=400,
            width=600
        )

        from lumen.dashboard import Layout
        return Layout(
            title='Cohort Retention Analysis',
            views=[heatmap, retention]
        )

# Custom tool: Forecast endpoint
def get_forecast(metric: str, periods: int = 12) -> dict:
    """
    Get ML forecast for a business metric.

    Args:
        metric: Metric name (revenue, customers, orders)
        periods: Number of future periods

    Returns:
        Forecast data with confidence intervals
    """
    # Simplified - would call ML service
    return {
        "metric": metric,
        "periods": periods,
        "forecast": [/* forecast values */],
        "lower_bound": [/* lower CI */],
        "upper_bound": [/* upper CI */]
    }

# Create UI
ui = lmai.ExplorerUI(
    source=source,

    # Select agents
    agents=[
        lmai.agents.TableListAgent,
        lmai.agents.ChatAgent,
        lmai.agents.SQLAgent,
        lmai.agents.hvPlotAgent,
        lmai.agents.AnalysisAgent(analyses=[CohortAnalysis])
    ],

    # Add tools
    tools=[
        lmai.tools.DocumentLookup,
        lmai.tools.FunctionTool(func=get_forecast)
    ],

    # UI customization
    title="Business Analytics AI",
    accent_color="#00aa41",

    # Coordinator
    coordinator="dependency"
)

ui.servable()
```

**Usage examples**:
- "Show me total revenue by month for Q4"
- "What's our customer retention rate?"
- "Create a cohort analysis" (triggers CohortAnalysis)
- "Forecast revenue for next 6 months" (uses forecast tool)
- "What does ARR mean?" (searches documents)

## Best Practices

### 1. LLM Selection

**For production**:
- **GPT-4o**: Best overall performance, balanced cost
- **Claude 3.5 Sonnet**: Excellent reasoning, SQL generation
- **Claude 3 Haiku**: Fast, cost-effective for simple queries

**For development**:
- **GPT-4o-mini**: Fast, cheap, good enough for testing
- **Local models**: No API costs, full privacy

**For sensitive data**:
- **Ollama + Llama 3.1**: Full local deployment
- **Azure OpenAI**: Enterprise compliance

### 2. Prompt Engineering

**Clear agent purposes**:
```python
purpose = """
Use this agent when the user asks about customer segmentation,
RFM analysis, or lifetime value calculations.

DO NOT use for general statistics or simple aggregations.
"""
```

**Structured system prompts**:
```python
_prompts = {
    "system": """
    You are a sales analytics expert.

    When generating SQL:
    - Use consistent date formats (YYYY-MM-DD)
    - Round monetary values to 2 decimals
    - Include appropriate WHERE clauses for data quality

    When creating visualizations:
    - Use clear, descriptive titles
    - Add axis labels
    - Choose appropriate chart types
    """
}
```

### 3. Performance Optimization

**Use appropriate models**:
```python
# Fast queries: use smaller models
lmai.llm.model = "gpt-4o-mini"  # For simple questions

# Complex analysis: use powerful models
lmai.llm.model = "claude-3-5-sonnet-20241022"  # For complex reasoning
```

**Limit table sizes**:
```python
source = DuckDBSource(
    tables=["large_table.parquet"],
    table_kwargs={
        "large_table": {"nrows": 100000}  # Limit for AI exploration
    }
)
```

**Cache responses** (future feature):
```python
# Coming soon: response caching
ui = lmai.ExplorerUI(
    source=source,
    cache=True
)
```

### 4. Security and Privacy

**Environment variables for secrets**:
```bash
export OPENAI_API_KEY="..."
export DATABASE_URL="postgresql://..."
```

**Never commit API keys**:
```python
# Bad
lmai.llm.api_key = "sk-..."

# Good
import os
lmai.llm.api_key = os.getenv("OPENAI_API_KEY")
```

**Use local models for sensitive data**:
```python
# No data leaves your infrastructure
lmai.llm.llm_type = "ollama"
lmai.llm.model = "llama3.1"
```

**Row-level security** (database-level):
```sql
-- PostgreSQL RLS example
CREATE POLICY user_data ON sales
FOR SELECT
USING (user_id = current_user_id());
```

### 5. User Experience

**Provide example queries**:
```python
ui = lmai.ExplorerUI(
    source=source,
    suggestions=[
        "Show me revenue trends over the last 6 months",
        "What are the top 10 products by sales?",
        "Create a customer segmentation analysis",
        "Compare performance across regions"
    ]
)
```

**Clear error messages**:
```python
class CustomAgent(Agent):
    async def respond(self, query):
        try:
            result = self.process(query)
        except KeyError as e:
            yield f"‚ùå Column not found: {e}. Available columns: {self.columns}"
        except Exception as e:
            yield f"‚ùå Error: {str(e)}. Please rephrase your question."
```

**Show progress**:
```python
async def respond(self, query):
    yield "üîç Analyzing your question..."
    sql = await self.generate_sql(query)

    yield f"üìä Executing query:\n```sql\n{sql}\n```"
    result = self.execute(sql)

    yield "‚úÖ Complete!"
    yield result
```

## Deployment

### Development

```bash
# Local testing with auto-reload
lumen-ai serve app.py --autoreload --show

# With specific port
lumen-ai serve app.py --port 5007
```

### Production

```bash
# Production server
panel serve app.py \
  --port 80 \
  --num-procs 4 \
  --allow-websocket-origin=analytics.company.com

# With authentication
panel serve app.py \
  --oauth-provider=generic \
  --oauth-key=${OAUTH_KEY} \
  --oauth-secret=${OAUTH_SECRET}
```

### Docker

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application
COPY app.py .
COPY data/ ./data/

# Environment
ENV ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}

CMD ["panel", "serve", "app.py", "--port", "5006", "--address", "0.0.0.0"]
```

### Environment Variables

```bash
# LLM providers
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
MISTRAL_API_KEY=...

# Database
DATABASE_URL=postgresql://user:pass@host/db

# Application
LUMEN_TITLE="Analytics AI"
LUMEN_ACCENT_COLOR="#00aa41"
```

## Troubleshooting

### Issue: LLM Not Responding

**Check API key**:
```python
import os
print(os.getenv("OPENAI_API_KEY"))  # Should not be None
```

**Verify network connection**:
```bash
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

**Check rate limits**:
- OpenAI: Tier-based limits
- Anthropic: Request-per-minute limits
- Solution: Implement retry logic or upgrade tier

### Issue: Agent Not Selected

**Verify purpose is clear**:
```python
purpose = """
SPECIFIC use case when this agent should be used.
Clear keywords and patterns.
"""
```

**Check requires/provides**:
```python
# Agent won't run if requirements not met
requires = ["current_source"]  # Must be in memory
```

**Debug coordinator**:
```python
# See which agent was selected
print(ui.agent_manager.last_selected_agent)
```

### Issue: SQL Generation Errors

**Provide schema context**:
```python
# Include data dictionary as document
source = DuckDBSource(
    tables=["sales"],
    documents=["./schema_docs.md"]
)
```

**Show examples in prompts**:
```python
_prompts = {
    "system": """
    Generate SQL for DuckDB database.

    Example:
    User: "Show total sales by region"
    SQL: SELECT region, SUM(revenue) as total_sales
         FROM sales
         GROUP BY region
         ORDER BY total_sales DESC
    """
}
```

### Issue: Poor Visualization Quality

**Use VegaLiteAgent for polished output**:
```python
agents = [
    lmai.agents.VegaLiteAgent,  # Better for presentation
    # lmai.agents.hvPlotAgent   # Better for interaction
]
```

**Customize chart prompts**:
```python
class CustomPlotAgent(lmai.agents.hvPlotAgent):
    _prompts = {
        "system": """
        Create publication-quality visualizations.

        Always:
        - Add descriptive titles
        - Label axes clearly
        - Use appropriate color schemes
        - Include legends when needed
        - Set responsive=True for mobile
        """
    }
```

## Resources

- [Lumen AI Documentation](https://lumen.holoviz.org/lumen_ai/)
- [Lumen AI Getting Started](https://lumen.holoviz.org/lumen_ai/getting_started/using_lumen_ai.html)
- [Custom Agents Guide](https://lumen.holoviz.org/lumen_ai/how_to/custom_agents.html)
- [Custom Analyses Guide](https://lumen.holoviz.org/lumen_ai/how_to/custom_analyses.html)
- [Custom Tools Guide](https://lumen.holoviz.org/lumen_ai/how_to/custom_tools.html)
- [GitHub Repository](https://github.com/holoviz/lumen)
- [Community Discourse](https://discourse.holoviz.org)

## Use Cases

### Business Analytics
- Ad-hoc revenue analysis
- Customer behavior exploration
- Sales performance tracking
- Market segmentation

### Data Science
- Exploratory data analysis
- Quick statistical summaries
- Hypothesis testing
- Pattern discovery

### Operations
- Real-time monitoring queries
- Anomaly investigation
- Performance metrics
- Incident analysis

### Self-Service Analytics
- Enabling business users
- Reducing analyst backlog
- Democratizing data access
- Maintaining governance

## Summary

Lumen AI transforms data exploration through natural language:

**Strengths**:
- No SQL or coding required
- Flexible LLM support
- Extensible architecture
- Privacy-focused options
- Reduces analyst workload

**Ideal for**:
- Ad-hoc data exploration
- Non-technical users
- Rapid insights
- Self-service analytics

**Consider alternatives when**:
- Fixed dashboards needed ‚Üí Lumen Dashboards
- No LLM budget ‚Üí Traditional BI tools
- Highly custom logic ‚Üí Panel applications

For declarative dashboard building, see the **Lumen Dashboards** skill.
